{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944adfb9-a765-4183-ac07-70111aa2c550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd375ff6-849f-4e94-82ca-6156a7e1d05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491fefc-000a-48a2-8ee8-3855793f266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3a154-25b5-4db9-9d05-efbe7d73c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaed9fa-c47f-4bea-910f-d6630d353fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa494d0e-87ac-4dbb-9811-973c7049d967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2ac20-68f6-44b3-98f5-0cf7c228bf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce8987-2931-486b-9f1c-1e82d0ca2a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dbe22-66cc-4b3c-a4b9-adeb8c8db8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b14ee-3d32-4c38-a823-16220ccac243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a5524-f6e2-4188-ab60-ea1765df7097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8b361-b709-47a5-bf72-7c6c8b7508f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7bc7b-532e-420b-a4b4-7f97c5041237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcee12-6d39-404c-abcd-6895d4644c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101152c5-7d8e-4d85-8372-4bd510ca1d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e651c-f53e-4ae0-a0ed-3d70adf0299e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16fcc2-9e4a-4430-866d-048400a50a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d364a3e-1677-4635-b478-5070f19a2057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb84774-2a82-481c-b387-d308ddc60977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56d72ad4-a935-4f0b-84af-89b3d5886c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Path found by A*: ['A', 'B', 'D', 'E', 'F']\n",
      "Total Cost: 10\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import math\n",
    "\n",
    "def a_star(start, goal, graph, heuristic):\n",
    "    open_list = []\n",
    "    heapq.heappush(open_list, (0, start))\n",
    "\n",
    "    came_from = {}\n",
    "    g_score = {node: float('inf') for node in graph}\n",
    "    g_score[start] = 0\n",
    "\n",
    "    while open_list:\n",
    "        current_f, current = heapq.heappop(open_list)\n",
    "\n",
    "        if current == goal:\n",
    "            path = []\n",
    "            while current in came_from:\n",
    "                path.append(current)\n",
    "                current = came_from[current]\n",
    "            path.append(start)\n",
    "            path.reverse()\n",
    "            return path, g_score[goal]\n",
    "\n",
    "        for neighbor, cost in graph[current].items():\n",
    "            tentative_g = g_score[current] + cost\n",
    "            if tentative_g < g_score[neighbor]:\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g\n",
    "                f_score = tentative_g + heuristic[neighbor]\n",
    "                heapq.heappush(open_list, (f_score, neighbor))\n",
    "    return None, float('inf')\n",
    "\n",
    "graph = {\n",
    "    'A': {'B': 4, 'C': 3},\n",
    "    'B': {'A': 4, 'D': 2, 'E': 5},\n",
    "    'C': {'A': 3, 'D': 6},\n",
    "    'D': {'B': 2, 'C': 6, 'E': 1, 'F': 7},\n",
    "    'E': {'B': 5, 'D': 1, 'F': 3},\n",
    "    'F': {'D': 7, 'E': 3}\n",
    "}\n",
    "\n",
    "heuristic = {\n",
    "    'A': 10,\n",
    "    'B': 8,\n",
    "    'C': 9,\n",
    "    'D': 5,\n",
    "    'E': 3,\n",
    "    'F': 0\n",
    "}\n",
    "\n",
    "start = 'A'\n",
    "goal = 'F'\n",
    "path, cost = a_star(start, goal, graph, heuristic)\n",
    "\n",
    "print(\"Shortest Path found by A*:\", path)\n",
    "print(\"Total Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3adab9-ae06-4693-a0dc-ad4a605689f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path found using BFS:\n",
      "[(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (2, 3), (2, 4), (3, 4), (4, 4)]\n",
      "Total steps: 8\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def bfs_shortest_path(maze, start, goal):\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "    visited = [[False]*cols for _ in range(rows)]\n",
    "    parent = [[None]*cols for _ in range(rows)]\n",
    "\n",
    "    directions = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "\n",
    "    queue = deque([start])\n",
    "    visited[start[0]][start[1]] = True\n",
    "\n",
    "    while queue:\n",
    "        x, y = queue.popleft()\n",
    "\n",
    "        if (x, y) == goal:\n",
    "            path = []\n",
    "            pos = goal\n",
    "            while pos is not None:\n",
    "                path.append(pos)\n",
    "                pos = parent[pos[0]][pos[1]]\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and not visited[nx][ny] and maze[nx][ny] == 0:\n",
    "                visited[nx][ny] = True\n",
    "                parent[nx][ny] = (x, y)\n",
    "                queue.append((nx, ny))\n",
    "\n",
    "    return None  # No path found\n",
    "\n",
    "maze = [\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 0]\n",
    "]\n",
    "\n",
    "start = (0, 0)   # Starting cell\n",
    "goal = (4, 4)    # Goal cell\n",
    "\n",
    "path = bfs_shortest_path(maze, start, goal)\n",
    "\n",
    "if path:\n",
    "    print(\"Shortest path found using BFS:\")\n",
    "    print(path)\n",
    "    print(\"Total steps:\", len(path)-1)\n",
    "else:\n",
    "    print(\"No path found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88031c79-5640-4213-9f78-3cabf86c41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS Traversal Order (First exploration):\n",
      "Start → Forest → Cave → Treasure → River → Village → Castle → Dungeon → Market\n",
      "\n",
      "All possible paths from Start to Treasure:\n",
      "Path 1: Start → Forest → Cave → Treasure\n"
     ]
    }
   ],
   "source": [
    "game_map = {\n",
    "    'Start': ['Forest', 'Village'],\n",
    "    'Forest': ['Cave', 'River'],\n",
    "    'Village': ['Castle', 'Market'],\n",
    "    'Cave': ['Treasure'],\n",
    "    'River': [],\n",
    "    'Castle': ['Dungeon'],\n",
    "    'Market': [],\n",
    "    'Dungeon': [],\n",
    "    'Treasure': []\n",
    "}\n",
    "\n",
    "def dfs(graph, node, goal, visited, path, all_paths, order):\n",
    "    visited.add(node)\n",
    "    path.append(node)\n",
    "    order.append(node)  \n",
    "\n",
    "    if node == goal:\n",
    "        all_paths.append(list(path))\n",
    "    else:\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                # We need to make a copy of visited for path exploration\n",
    "                # In this specific recursive implementation, we add/remove\n",
    "                dfs(graph, neighbor, goal, visited, path, all_paths, order)\n",
    "            \n",
    "    path.pop()\n",
    "    visited.remove(node)\n",
    "\n",
    "# To find ALL paths correctly, visited must be handled on a per-path basis\n",
    "# The provided code finds paths but the 'visited' logic is flawed for finding *all* paths\n",
    "# A better approach for finding all paths:\n",
    "def find_all_paths_dfs(graph, node, goal, path, all_paths, order_visited):\n",
    "    path = path + [node]\n",
    "    \n",
    "    if node not in order_visited:\n",
    "        order_visited.append(node)\n",
    "\n",
    "    if node == goal:\n",
    "        all_paths.append(path)\n",
    "    \n",
    "    for neighbor in graph[node]:\n",
    "        if neighbor not in path: # Avoid cycles in the current path\n",
    "            find_all_paths_dfs(graph, neighbor, goal, path, all_paths, order_visited)\n",
    "\n",
    "start = 'Start'\n",
    "goal = 'Treasure'\n",
    "\n",
    "all_paths = []\n",
    "order = []\n",
    "path = []\n",
    "\n",
    "# Using the corrected function\n",
    "find_all_paths_dfs(game_map, start, goal, path, all_paths, order)\n",
    "\n",
    "print(\"DFS Traversal Order (First exploration):\")\n",
    "print(\" → \".join(order))\n",
    "\n",
    "print(\"\\nAll possible paths from Start to Treasure:\")\n",
    "for i, p in enumerate(all_paths, 1):\n",
    "    print(f\"Path {i}: {' → '.join(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60792a2-a4a6-4786-a481-b2c3eb156399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path found: [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
      "Total cost: 8\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def a_star(maze, start, goal):\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "    open_list = []\n",
    "    heapq.heappush(open_list, (0, start))\n",
    "    g_cost = {start: 0}\n",
    "    came_from = {start: None}\n",
    "\n",
    "    def heuristic(a, b):\n",
    "        return abs(a[0]-b[0]) + abs(a[1]-b[1])\n",
    "\n",
    "    while open_list:\n",
    "        _, current = heapq.heappop(open_list)\n",
    "        if current == goal:\n",
    "            path = []\n",
    "            while current is not None:\n",
    "                path.append(current)\n",
    "                current = came_from[current]\n",
    "            return path[::-1]\n",
    "        x, y = current\n",
    "        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols and maze[nx][ny] == 0:\n",
    "                new_cost = g_cost[current] + 1\n",
    "                neighbor = (nx, ny)\n",
    "                if neighbor not in g_cost or new_cost < g_cost[neighbor]:\n",
    "                    g_cost[neighbor] = new_cost\n",
    "                    f_cost = new_cost + heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_cost, neighbor))\n",
    "                    came_from[neighbor] = current\n",
    "    return None\n",
    "\n",
    "maze = [\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "path = a_star(maze, start, goal)\n",
    "\n",
    "if path:\n",
    "    print(\"Path found:\", path)\n",
    "    print(\"Total cost:\", len(path)-1)\n",
    "else:\n",
    "    print(\"No path found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5f65c8-dd8d-4bae-b417-ae60fd3b6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution found:\n",
      "(1, 2, 3)\n",
      "(4, 0, 6)\n",
      "(7, 5, 8)\n",
      "-----\n",
      "(1, 2, 3)\n",
      "(4, 5, 6)\n",
      "(7, 0, 8)\n",
      "-----\n",
      "(1, 2, 3)\n",
      "(4, 5, 6)\n",
      "(7, 8, 0)\n",
      "-----\n",
      "Total Steps: 2\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def heuristic(state, goal):\n",
    "    return sum(abs(s//3 - g//3) + abs(s%3 - g%3)\n",
    "               for s, g in ((state.index(i), goal.index(i)) for i in range(1,9)))\n",
    "\n",
    "def get_neighbors(state):\n",
    "    neighbors = []\n",
    "    i = state.index(0)\n",
    "    x, y = divmod(i, 3)\n",
    "    moves = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "    for dx, dy in moves:\n",
    "        nx, ny = x+dx, y+dy\n",
    "        if 0 <= nx < 3 and 0 <= ny < 3:\n",
    "            ni = nx*3+ny\n",
    "            new_state = list(state)\n",
    "            new_state[i], new_state[ni] = new_state[ni], new_state[i]\n",
    "            neighbors.append(tuple(new_state))\n",
    "    return neighbors\n",
    "\n",
    "def a_star_8_puzzle(start, goal):\n",
    "    open_list = []\n",
    "    heapq.heappush(open_list, (heuristic(start, goal), 0, start))\n",
    "    came_from = {start: None}\n",
    "    g_cost = {start: 0}\n",
    "    \n",
    "    closed_set = set()\n",
    "\n",
    "    while open_list:\n",
    "        f, cost, current = heapq.heappop(open_list)\n",
    "        \n",
    "        if current in closed_set:\n",
    "            continue\n",
    "        closed_set.add(current)\n",
    "        \n",
    "        if current == goal:\n",
    "            path = []\n",
    "            while current:\n",
    "                path.append(current)\n",
    "                current = came_from[current]\n",
    "            return path[::-1]\n",
    "            \n",
    "        for neighbor in get_neighbors(current):\n",
    "            if neighbor in closed_set:\n",
    "                continue\n",
    "                \n",
    "            new_cost = g_cost[current] + 1\n",
    "            if neighbor not in g_cost or new_cost < g_cost[neighbor]:\n",
    "                g_cost[neighbor] = new_cost\n",
    "                f_cost = new_cost + heuristic(neighbor, goal)\n",
    "                heapq.heappush(open_list, (f_cost, new_cost, neighbor))\n",
    "                came_from[neighbor] = current\n",
    "    return None\n",
    "\n",
    "start = (1,2,3,4,0,6,7,5,8)\n",
    "goal = (1,2,3,4,5,6,7,8,0)\n",
    "path = a_star_8_puzzle(start, goal)\n",
    "\n",
    "if path:\n",
    "    print(\"Solution found:\")\n",
    "    for step in path:\n",
    "        print(step[:3])\n",
    "        print(step[3:6])\n",
    "        print(step[6:])\n",
    "        print(\"-\" * 5)\n",
    "    print(\"Total Steps:\", len(path)-1)\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1761351d-56c8-49a3-820d-cbd8f5cf5723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for X:  0\n",
      "Enter col (0-2) for X:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  0\n",
      "Enter col (0-2) for O:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell taken, try again\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  0\n",
      "Enter col (0-2) for O:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell taken, try again\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  7\n",
      "Enter col (0-2) for O:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates out of bounds. Try again.\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  7\n",
      "Enter col (0-2) for O:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates out of bounds. Try again.\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  7\n",
      "Enter col (0-2) for O:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Please enter a number (0-2).\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter row (0-2) for O:  2\n",
      "Enter col (0-2) for O:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m print_board(board)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     r = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEnter row (0-2) for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     27\u001b[39m     c = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnter col (0-2) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m0\u001b[39m <= r <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m <= c <= \u001b[32m2\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py:1396\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1394\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_shell_context_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py:1441\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1440\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def print_board(b):\n",
    "    for r in b:\n",
    "        print(\" | \".join(r))\n",
    "        print(\"-----\")\n",
    "\n",
    "def check_winner(b):\n",
    "    for r in b:\n",
    "        if r[0] == r[1] == r[2] != \" \":\n",
    "            return r[0]\n",
    "    for c in range(3):\n",
    "        if b[0][c] == b[1][c] == b[2][c] != \" \":\n",
    "            return b[0][c]\n",
    "    if b[0][0] == b[1][1] == b[2][2] != \" \":\n",
    "        return b[0][0]\n",
    "    if b[0][2] == b[1][1] == b[2][0] != \" \":\n",
    "        return b[0][2]\n",
    "    return None\n",
    "\n",
    "board = [[\" \"]*3 for _ in range(3)]\n",
    "player = \"X\"\n",
    "turns = 0\n",
    "\n",
    "while turns < 9:\n",
    "    print_board(board)\n",
    "    try:\n",
    "        r = int(input(f\"Enter row (0-2) for {player}: \"))\n",
    "        c = int(input(f\"Enter col (0-2) for {player}: \"))\n",
    "        \n",
    "        if not (0 <= r <= 2 and 0 <= c <= 2):\n",
    "            print(\"Coordinates out of bounds. Try again.\")\n",
    "            continue\n",
    "            \n",
    "        if board[r][c] == \" \":\n",
    "            board[r][c] = player\n",
    "            turns += 1\n",
    "            winner = check_winner(board)\n",
    "            if winner:\n",
    "                print_board(board)\n",
    "                print(f\"Winner is {winner}\")\n",
    "                break\n",
    "            player = \"O\" if player == \"X\" else \"X\"\n",
    "        else:\n",
    "            print(\"Cell taken, try again\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number (0-2).\")\n",
    "\n",
    "else:\n",
    "    print_board(board)\n",
    "    print(\"It's a Draw!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f6bda5-f6f9-4910-bbf6-f58b32ee01c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number of disks:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move disk 1 from A to B\n",
      "Move disk 2 from A to C\n",
      "Move disk 1 from B to C\n"
     ]
    }
   ],
   "source": [
    "def tower_of_hanoi(n, source, target, auxiliary):\n",
    "    if n == 1:\n",
    "        print(f\"Move disk 1 from {source} to {target}\")\n",
    "        return\n",
    "    tower_of_hanoi(n-1, source, auxiliary, target)\n",
    "    print(f\"Move disk {n} from {source} to {target}\")\n",
    "    tower_of_hanoi(n-1, auxiliary, target, source)\n",
    "\n",
    "try:\n",
    "    n = int(input(\"Enter number of disks: \"))\n",
    "    if n <= 0:\n",
    "        print(\"Please enter a positive number of disks.\")\n",
    "    else:\n",
    "        tower_of_hanoi(n, 'A', 'C', 'B')\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d78e3c-5b6a-4b45-b008-4b33d8553ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached!\n",
      "Path of states (jug1, jug2):\n",
      "(0, 0)\n",
      "(0, 3)\n",
      "(3, 0)\n",
      "(3, 3)\n",
      "(4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 3), (3, 0), (3, 3), (4, 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def water_jug(jug1_cap, jug2_cap, target):\n",
    "    visited = set()\n",
    "    q = deque([((0, 0), [(0, 0)])]) # Store (state, path)\n",
    "\n",
    "    while q:\n",
    "        (a, b), path = q.popleft()\n",
    "        \n",
    "        if (a, b) in visited:\n",
    "            continue\n",
    "        visited.add((a, b))\n",
    "\n",
    "        if a == target or b == target:\n",
    "            print(\"Goal reached!\")\n",
    "            print(\"Path of states (jug1, jug2):\")\n",
    "            for state in path:\n",
    "                print(state)\n",
    "            return path\n",
    "\n",
    "        # 1. Fill jug1\n",
    "        q.append(((jug1_cap, b), path + [(jug1_cap, b)]))\n",
    "        # 2. Fill jug2\n",
    "        q.append(((a, jug2_cap), path + [(a, jug2_cap)]))\n",
    "        # 3. Empty jug1\n",
    "        q.append(((0, b), path + [(0, b)]))\n",
    "        # 4. Empty jug2\n",
    "        q.append(((a, 0), path + [(a, 0)]))\n",
    "        # 5. Pour jug1 to jug2\n",
    "        pour_to_2 = min(a, jug2_cap - b)\n",
    "        q.append(((a - pour_to_2, b + pour_to_2), path + [(a - pour_to_2, b + pour_to_2)]))\n",
    "        # 6. Pour jug2 to jug1\n",
    "        pour_to_1 = min(b, jug1_cap - a)\n",
    "        q.append(((a + pour_to_1, b - pour_to_1), path + [(a + pour_to_1, b - pour_to_1)]))\n",
    "        \n",
    "    print(\"Goal cannot be reached.\")\n",
    "    return None\n",
    "\n",
    "# Solve the 4-gallon jug, 3-gallon jug, get 2 gallons problem\n",
    "water_jug(4, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ecba66-943d-4321-97fa-8869492045bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"uber.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'uber.csv' not found.\")\n",
    "    print(\"Please download the dataset and place it in the same directory.\")\n",
    "    exit()\n",
    "    \n",
    "df = df.drop_duplicates().dropna()\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "target_col = None\n",
    "for col in ['fare_amount', 'price', 'amount', 'fare']:\n",
    "    if col in numeric_df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "if target_col is None:\n",
    "    # Fallback if no obvious target column\n",
    "    if numeric_df.shape[1] > 0:\n",
    "        target_col = numeric_df.columns[0]\n",
    "    else:\n",
    "        raise ValueError(\"No numeric columns found in dataset.\")\n",
    "\n",
    "X = numeric_df.drop(columns=[target_col])\n",
    "y = numeric_df[target_col]\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"Target Variable:\", target_col)\n",
    "print(\"Feature Columns:\", list(X.columns))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.histplot(y, kde=True, bins=30, color='skyblue')\n",
    "plt.title(f\"Distribution of {target_col}\")\n",
    "plt.xlabel(target_col)\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train_scaled, y_train)\n",
    "y_pred1 = model1.predict(X_test_scaled)\n",
    "\n",
    "r2_no_pca = r2_score(y_test, y_pred1)\n",
    "rmse_no_pca = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(range(1, len(explained) + 1), explained, marker='o')\n",
    "plt.axhline(0.95, color='r', linestyle='--')\n",
    "plt.title(\"Cumulative Explained Variance by PCA\")\n",
    "plt.xlabel(\"No. of Components\")\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "n_components_95 = int(np.searchsorted(explained, 0.95) + 1)\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "pca = PCA(n_components=n_components_95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train_pca, y_train)\n",
    "y_pred2 = model2.predict(X_test_pca)\n",
    "\n",
    "r2_pca = r2_score(y_test, y_pred2)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Without PCA\", \"With PCA\"],\n",
    "    \"R² Score\": [r2_no_pca, r2_pca],\n",
    "    \"RMSE\": [rmse_no_pca, rmse_pca]\n",
    "})\n",
    "print(\"\\nModel Performance Comparison:\\n\", comparison)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=comparison.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Value\"),\n",
    "            x=\"Metric\", y=\"Value\", hue=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison (With vs Without PCA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3131c29c-e6fa-476a-a8dd-e36786ea88df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"uber.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'uber.csv' not found.\")\n",
    "    print(\"Please download the dataset and place it in the same directory.\")\n",
    "    exit()\n",
    "    \n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "df = df.drop_duplicates().dropna()\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "target_col = None\n",
    "for col in ['fare_amount', 'price', 'amount', 'fare']:\n",
    "    if col in numeric_df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "if target_col is None:\n",
    "    if numeric_df.shape[1] > 0:\n",
    "        target_col = numeric_df.columns[0]\n",
    "    else:\n",
    "        raise ValueError(\"No numeric columns found in dataset.\")\n",
    "\n",
    "X = numeric_df.drop(columns=[target_col])\n",
    "y = numeric_df[target_col]\n",
    "\n",
    "print(f\"Target Variable: {target_col}\")\n",
    "\n",
    "print(\"\\nSummary Statistics:\\n\", df.describe())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.histplot(y, bins=30, kde=True, color='skyblue')\n",
    "plt.title(f\"Distribution of {target_col}\")\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_no_pca = LinearRegression()\n",
    "model_no_pca.fit(X_train_scaled, y_train)\n",
    "y_pred_no_pca = model_no_pca.predict(X_test_scaled)\n",
    "\n",
    "r2_no_pca = r2_score(y_test, y_pred_no_pca)\n",
    "rmse_no_pca = np.sqrt(mean_squared_error(y_test, y_pred_no_pca))\n",
    "mae_no_pca = mean_absolute_error(y_test, y_pred_no_pca)\n",
    "\n",
    "print(\"\\n=== Model Without PCA ===\")\n",
    "print(f\"R² Score: {r2_no_pca:.4f}\")\n",
    "print(f\"RMSE: {rmse_no_pca:.4f}\")\n",
    "print(f\"MAE: {mae_no_pca:.4f}\")\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_train_scaled)\n",
    "explained_cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(range(1, len(explained_cumsum) + 1), explained_cumsum, marker='o')\n",
    "plt.axhline(0.95, color='red', linestyle='--', label='95% Variance')\n",
    "plt.title(\"Cumulative Explained Variance by PCA Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "n_components_95 = int(np.searchsorted(explained_cumsum, 0.95) + 1)\n",
    "print(f\"\\nNumber of components to retain 95% variance: {n_components_95}\")\n",
    "\n",
    "pca = PCA(n_components=n_components_95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "model_pca = LinearRegression()\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "\n",
    "print(\"\\n=== Model With PCA ===\")\n",
    "print(f\"R² Score: {r2_pca:.4f}\")\n",
    "print(f\"RMSE: {rmse_pca:.4f}\")\n",
    "print(f\"MAE: {mae_pca:.4f}\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Without PCA\", \"With PCA\"],\n",
    "    \"R² Score\": [r2_no_pca, r2_pca],\n",
    "    \"RMSE\": [rmse_no_pca, rmse_pca],\n",
    "    \"MAE\": [mae_no_pca, mae_pca]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\\n\", comparison)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=comparison.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\"),\n",
    "            x=\"Metric\", y=\"Score\", hue=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison (With vs Without PCA)\")\n",
    "plt.ylabel(\"Score Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a04a26-207e-46f7-8d34-b736db88bf64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"house.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'house.csv' not found.\")\n",
    "    print(\"Please download a suitable house price dataset and ensure columns match.\")\n",
    "    exit()\n",
    "\n",
    "# Simple check for required columns\n",
    "required_cols = [\"Id\", \"Price\", \"Location\", \"Condition\", \"Garage\"]\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    print(f\"Error: Dataset must contain {required_cols}\")\n",
    "    # Making up data if file is missing/wrong\n",
    "    data = {\n",
    "        \"Id\": range(100),\n",
    "        \"Price\": np.random.randint(100000, 500000, 100),\n",
    "        \"Location\": np.random.choice([\"Urban\", \"Suburban\", \"Rural\"], 100),\n",
    "        \"Condition\": np.random.choice([\"Good\", \"Fair\", \"Poor\"], 100),\n",
    "        \"Garage\": np.random.choice([\"Yes\", \"No\"], 100),\n",
    "        \"Area\": np.random.randint(1000, 3000, 100),\n",
    "        \"Bedrooms\": np.random.randint(2, 6, 100)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Using dummy data as 'house.csv' was not found or valid.\")\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"Id\", \"Price\"])\n",
    "y = df[\"Price\"].values\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_features = [\"Location\", \"Condition\", \"Garage\"]\n",
    "# Identify numerical columns\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('regressor', LinearRegression())])\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "mses = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmses = np.sqrt(mses)\n",
    "r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"5-Fold MSEs:\", np.round(mses, 2))\n",
    "print(\"Mean MSE:\", np.round(mses.mean(), 2))\n",
    "print(\"5-Fold RMSEs:\", np.round(rmses, 2))\n",
    "print(\"Mean RMSE:\", np.round(rmses.mean(), 2))\n",
    "print(\"5-Fold R² Scores:\", np.round(r2_scores, 4))\n",
    "print(\"Mean R² Score:\", np.round(r2_scores.mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df3ccaf-cec0-419f-9757-f6a173205089",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"student.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'student.csv' not found. Using dummy data.\")\n",
    "    data = {\n",
    "        \"student_id\": range(1, 21),\n",
    "        \"hours_studied\": np.random.rand(20) * 10 + 1,\n",
    "        \"exam_score\": np.zeros(20) # will be filled\n",
    "    }\n",
    "    # Create a linear relationship with noise\n",
    "    data[\"exam_score\"] = data[\"hours_studied\"] * 8.5 + 15 + np.random.randn(20) * 5\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "if \"hours_studied\" not in df.columns or \"exam_score\" not in df.columns:\n",
    "     raise ValueError(\"CSV must contain 'hours_studied' and 'exam_score' columns.\")\n",
    "\n",
    "X = df[\"hours_studied\"].values\n",
    "y = df[\"exam_score\"].values\n",
    "\n",
    "mean_x = np.mean(X)\n",
    "mean_y = np.mean(y)\n",
    "\n",
    "num = np.sum((X - mean_x) * (y - mean_y))\n",
    "den = np.sum((X - mean_x) ** 2)\n",
    "\n",
    "m = num / den\n",
    "c = mean_y - m * mean_x\n",
    "\n",
    "def predict(x):\n",
    "    return m * x + c\n",
    "\n",
    "y_pred = predict(X)\n",
    "\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "ss_res = np.sum((y - y_pred) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"Linear Regression from Scratch\")\n",
    "print(f\"Formula: score = {m:.4f} * hours + {c:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Slope (m):\", round(m, 4))\n",
    "print(\"Intercept (c):\", round(c, 4))\n",
    "print(\"Mean Squared Error (MSE):\", round(mse, 4))\n",
    "print(\"R² Score:\", round(r2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb70647-44f5-4e32-8116-49863b21285c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KFold, cross_val_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"student.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'student.csv' not found. Using dummy data.\")\n",
    "    data = {\n",
    "        \"hours_studied\": np.random.rand(50) * 10 + 1,\n",
    "        \"sleep_hours\": np.random.rand(50) * 3 + 5,\n",
    "        \"attendance_percent\": np.random.rand(50) * 30 + 70,\n",
    "        \"Internal_marks\": np.random.rand(50) * 20 + 10,\n",
    "        \"exam_score\": np.zeros(50)\n",
    "    }\n",
    "    data[\"exam_score\"] = (data[\"hours_studied\"]*3 + \n",
    "                          data[\"attendance_percent\"]*0.5 + \n",
    "                          data[\"Internal_marks\"]*1.5 + \n",
    "                          np.random.randn(50)*3)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "features = [\"hours_studied\", \"sleep_hours\", \"attendance_percent\", \"Internal_marks\"]\n",
    "target = \"exam_score\"\n",
    "\n",
    "if not all(col in df.columns for col in features + [target]):\n",
    "    raise ValueError(\"CSV missing required columns.\")\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "model = LinearRegression()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "r2_scores = cross_val_score(model, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "print(\"K-Fold Cross-Validation for Student Scores\")\n",
    "print(\"R² Scores for each fold:\", np.round(r2_scores, 4))\n",
    "print(\"Average R² Score:\", round(np.mean(r2_scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f47a1d6-47c4-4783-b206-ae17c71a6ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KFold, cross_val_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"salary.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'salary.csv' not found. Using dummy data.\")\n",
    "    data = {\n",
    "        'Age': np.random.randint(22, 60, 100),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], 100),\n",
    "        'Education Level': np.random.choice([\"Bachelor's\", \"Master's\", \"PhD\"], 100),\n",
    "        'Job Title': np.random.choice(['Developer', 'Manager', 'Analyst'], 100),\n",
    "        'Years of Experience': np.random.randint(0, 30, 100),\n",
    "        'Salary': np.zeros(100)\n",
    "    }\n",
    "    data['Salary'] = 50000 + data['Years of Experience']*2000 + np.random.randn(100)*5000\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(columns=[\"Salary\"])\n",
    "y = df[\"Salary\"]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "# Identify numerical columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('regressor', LinearRegression())])\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores = cross_val_score(model, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "print(\"K-Fold Cross-Validation for IT Salaries\")\n",
    "print(\"R² Scores for each fold:\", np.round(r2_scores, 4))\n",
    "print(\"Average R² Score:\", round(np.mean(r2_scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599129ef-cc54-47b2-95d8-8c66550a58d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KFold, cross_val_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data provided in the prompt\n",
    "data = {\n",
    "    \"ad_spend\": [1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,\n",
    "                 1200,1700,2200,2700,3200,3700,4200,4700,5200,5700],\n",
    "    \"discount\": [5,10,0,15,20,5,10,25,30,15,\n",
    "                 8,12,3,18,22,7,14,28,33,17],\n",
    "    \"customer_footfall\": [200,250,300,350,400,450,500,550,600,650,\n",
    "                          210,260,310,360,410,460,510,560,610,660],\n",
    "    \"sales\": [10000,15000,13000,20000,25000,22000,27000,30000,35000,40000,\n",
    "              11000,16000,14000,21000,26000,23000,28000,31000,36000,41000]\n",
    "}\n",
    "# Increased N to 20 for KFold=5 to work\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[[\"ad_spend\", \"discount\", \"customer_footfall\"]]\n",
    "y = df[\"sales\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "# n_splits=5 requires at least 5 samples. \n",
    "# We set n_splits=4 because the data (N=10) is small\n",
    "# Updated N=20, so n_splits=5 is fine.\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "print(\"K-Fold Cross-Validation for Sales Forecast\")\n",
    "print(\"R² Scores for each fold:\", np.round(scores, 4))\n",
    "print(\"Average R²:\", np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e12e7f59-a27a-4011-8e7c-a6228796def3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"emails.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'emails.csv' not found. Using dummy data.\")\n",
    "    # Create dummy word frequency data\n",
    "    N = 1000\n",
    "    data = {f\"word_{i}\": np.random.randint(0, 5, N) for i in range(50)}\n",
    "    data[\"Prediction\"] = np.random.choice([0, 1], N, p=[0.7, 0.3])\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Drop any non-numeric columns (like \"Email No.\" or text columns)\n",
    "df = df.select_dtypes(include=[\"number\"])\n",
    "df = df.dropna()\n",
    "\n",
    "if \"Prediction\" not in df.columns:\n",
    "    raise ValueError(\"The dataset must have a 'Prediction' column as the target variable.\")\n",
    "\n",
    "X = df.drop(columns=[\"Prediction\"])\n",
    "y = df[\"Prediction\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== Naïve Bayes Email Spam Detection ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Spam', 'Spam'], \n",
    "            yticklabels=['Not Spam', 'Spam'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2e1f0a-ac43-4152-b3ef-b47e4da6fdb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"emails.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'emails.csv' not found. Using dummy data.\")\n",
    "    N = 1000\n",
    "    data = {f\"word_{i}\": np.random.randint(0, 5, N) for i in range(50)}\n",
    "    data[\"Prediction\"] = np.random.choice([0, 1], N, p=[0.7, 0.3])\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
    "\n",
    "if \"Prediction\" not in df.columns:\n",
    "    raise ValueError(\"The dataset must have a 'Prediction' column.\")\n",
    "\n",
    "X = df.drop(columns=[\"Prediction\"])\n",
    "y = df[\"Prediction\"].values\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "\n",
    "def fit_naive_bayes(X, y):\n",
    "    classes = np.unique(y)\n",
    "    class_prob = {}\n",
    "    feature_prob = {}\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        class_prob[c] = X_c.shape[0] / X.shape[0]\n",
    "        # Laplace (add-1) smoothing\n",
    "        feature_prob[c] = (X_c.sum(axis=0) + 1) / (X_c.sum() + n_features)\n",
    "    return classes, class_prob, feature_prob\n",
    "\n",
    "def predict_naive_bayes(X, classes, class_prob, feature_prob):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        posteriors = []\n",
    "        for c in classes:\n",
    "            # Use log probabilities to avoid underflow\n",
    "            log_prior = np.log(class_prob[c])\n",
    "            # Handle cases where feature_prob[c] is 0 (though smoothing should prevent this)\n",
    "            log_likelihood = np.sum(np.log(feature_prob[c] + 1e-9) * x)\n",
    "            posteriors.append(log_prior + log_likelihood)\n",
    "        preds.append(classes[np.argmax(posteriors)])\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "classes, class_prob, feature_prob = fit_naive_bayes(X_train, y_train)\n",
    "y_pred = predict_naive_bayes(X_test, classes, class_prob, feature_prob)\n",
    "\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"=== Naïve Bayes (From Scratch) ===\")\n",
    "print(\"Email Spam Detection Accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ec320b-a4e6-4947-8945-0ae87011dd7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"emails.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'emails.csv' not found. Using dummy data.\")\n",
    "    N = 1000\n",
    "    data = {f\"word_{i}\": np.random.randint(0, 5, N) for i in range(50)}\n",
    "    data[\"Prediction\"] = np.random.choice([0, 1], N, p=[0.85, 0.15]) # Imbalanced\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
    "\n",
    "if \"Prediction\" not in df.columns:\n",
    "    raise ValueError(\"The dataset must have a 'Prediction' column.\")\n",
    "    \n",
    "X = df.drop(columns=[\"Prediction\"])\n",
    "y = df[\"Prediction\"]\n",
    "\n",
    "print(\"Class Distribution Before Balancing:\\n\", y.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE (oversampling minority class)\n",
    "# Note: SMOTE works best on non-sparse data.\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass Distribution After SMOTE:\\n\", pd.Series(y_train_res).value_counts())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM Model\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== SVM Email Spam Detection Results (with SMOTE) ===\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Normal\", \"Spam\"], \n",
    "            yticklabels=[\"Normal\", \"Spam\"])\n",
    "plt.title(\"SVM Confusion Matrix (SMOTE)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa41eea2-b32c-4078-baff-e74bdfaff2b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"emails.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'emails.csv' not found. Using dummy data.\")\n",
    "    N = 1000\n",
    "    data = {f\"word_{i}\": np.random.randint(0, 5, N) for i in range(50)}\n",
    "    data[\"Prediction\"] = np.random.choice([0, 1], N, p=[0.7, 0.3])\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
    "\n",
    "if \"Prediction\" not in df.columns:\n",
    "    raise ValueError(\"The dataset must have a 'Prediction' column.\")\n",
    "\n",
    "X = df.drop(columns=[\"Prediction\"]).values\n",
    "y = df[\"Prediction\"].values\n",
    "\n",
    "# SVM requires labels -1 and 1\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "# Standardize\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-6)\n",
    "\n",
    "\n",
    "split = int(0.7 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "class FastSVM:\n",
    "    def __init__(self, lr=0.0001, lambda_param=0.01, epochs=300):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            margin = y * (np.dot(X, self.w) - self.b)\n",
    "            misclassified = margin < 1\n",
    "            \n",
    "            # Hinge Loss Gradient\n",
    "            # dw = lambda*w - y*x (if misclassified)\n",
    "            # db = -y (if misclassified)\n",
    "            \n",
    "            dw = (self.lambda_param * self.w) - np.dot(X[misclassified].T, y[misclassified])\n",
    "            db = -np.sum(y[misclassified])\n",
    "\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)\n",
    "\n",
    "model = FastSVM(lr=0.00001, lambda_param=0.01, epochs=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "tp = np.sum((y_test == 1) & (y_pred == 1))\n",
    "tn = np.sum((y_test == -1) & (y_pred == -1))\n",
    "fp = np.sum((y_test == -1) & (y_pred == 1))\n",
    "fn = np.sum((y_test == 1) & (y_pred == -1))\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "precision = tp / (tp + fp + 1e-6)\n",
    "recall = tp / (tp + fn + 1e-6)\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "\n",
    "print(\"=== SVM (From Scratch) Email Spam Detection ===\")\n",
    "print(f\"Confusion Matrix:\\n[[TP={tp}, FP={fp}], [FN={fn}, TN={tn}]]\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47ff0a7-bf3d-4ecb-83af-97d6caf5d872",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"performance.csv\").dropna()\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'performance.csv' not found. Using dummy data.\")\n",
    "    N = 100\n",
    "    data = {\n",
    "        \"Study_Hours_per_Week\": np.random.rand(N) * 15 + 1,\n",
    "        \"Attendance_Rate\": np.random.rand(N) * 30 + 70,\n",
    "        \"Internal_Scores\": np.random.rand(N) * 40 + 60,\n",
    "        \"Pass_Fail\": np.random.choice([\"Pass\", \"Fail\"], N)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = [\"Study_Hours_per_Week\", \"Attendance_Rate\", \"Internal_Scores\", \"Pass_Fail\"]\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    raise ValueError(\"CSV missing required columns.\")\n",
    "\n",
    "X = df[[\"Study_Hours_per_Week\", \"Attendance_Rate\", \"Internal_Scores\"]].values.astype(float)\n",
    "y = np.where(df[\"Pass_Fail\"] == \"Pass\", 1, -1)\n",
    "\n",
    "# Standardize\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-6)\n",
    "\n",
    "split = int(0.7 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "def polynomial_kernel(X1, X2, degree=2, coef0=1):\n",
    "    return np.power(np.dot(X1, X2.T) + coef0, degree)\n",
    "\n",
    "class PolynomialSVM:\n",
    "    def __init__(self, C=1.0, lr=0.001, n_iters=300, degree=2, coef0=1):\n",
    "        self.C = C\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.alpha = None\n",
    "        self.b = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        self.alpha = np.zeros(n)\n",
    "        self.b = 0\n",
    "        K = polynomial_kernel(X, X, self.degree, self.coef0)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            margin = np.dot((self.alpha * y), K) + self.b\n",
    "            \n",
    "            for i in range(n):\n",
    "                condition = y[i] * margin[i] < 1\n",
    "                \n",
    "                # Gradient of the dual loss w.r.t alpha[i]\n",
    "                # We use a simplified (primal) gradient descent on alphas (not exactly SMO)\n",
    "                # This is a Subgradient method for the dual\n",
    "                grad = 1 - (y[i] * margin[i]) # Hinge loss part\n",
    "                \n",
    "                if condition:\n",
    "                    # Update alpha[i] based on hinge loss\n",
    "                    self.alpha[i] += self.lr * (1 - self.C * self.alpha[i])\n",
    "                else:\n",
    "                    # Regularization part\n",
    "                    self.alpha[i] += self.lr * (-self.C * self.alpha[i])\n",
    "\n",
    "                self.alpha[i] = np.clip(self.alpha[i], 0, self.C) # Box constraint\n",
    "\n",
    "            # Update bias (simplified)\n",
    "            self.b -= self.lr * np.mean(y * (margin < 1))\n",
    "\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def project(self, X):\n",
    "        K = polynomial_kernel(X, self.X, self.degree, self.coef0)\n",
    "        return np.dot(self.alpha * self.y, K.T) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n",
    "\n",
    "model = PolynomialSVM(C=1.0, lr=0.0001, n_iters=500, degree=2, coef0=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "tp = np.sum((y_test == 1) & (y_pred == 1))\n",
    "tn = np.sum((y_test == -1) & (y_pred == -1))\n",
    "fp = np.sum((y_test == -1) & (y_pred == 1))\n",
    "fn = np.sum((y_test == 1) & (y_pred == -1))\n",
    "\n",
    "precision = tp / (tp + fp + 1e-6)\n",
    "recall = tp / (tp + fn + 1e-6)\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "\n",
    "print(\"=== Polynomial SVM (From Scratch) - Student Performance ===\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n[[TP={tp}, FP={fp}], [FN={fn}, TN={tn}]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d21d6e3-c953-4572-af5a-c2947e27ca3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"cancer.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'cancer.csv' not found. Using dummy data.\")\n",
    "    N = 500\n",
    "    data = {f\"feature_{i}\": np.random.rand(N) for i in range(30)}\n",
    "    data['id'] = range(N)\n",
    "    data['diagnosis'] = np.random.choice(['M', 'B'], N)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df = df.drop(columns=[\"id\"], errors='ignore').dropna()\n",
    "\n",
    "if \"diagnosis\" not in df.columns:\n",
    "    raise ValueError(\"CSV must contain a 'diagnosis' column (M/B).\")\n",
    "\n",
    "y = np.where(df[\"diagnosis\"] == \"M\", 1, -1)\n",
    "X = df.drop(columns=[\"diagnosis\"]).values.astype(float)\n",
    "\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-6)\n",
    "\n",
    "split_idx = int(0.7 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "def polynomial_kernel(X1, X2, degree=3, coef0=1):\n",
    "    return (np.dot(X1, X2.T) + coef0) ** degree\n",
    "\n",
    "class PolynomialSVM:\n",
    "    def __init__(self, C=1.0, lr=0.001, n_iters=300, degree=3, coef0=1):\n",
    "        self.C = C\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.alpha = None\n",
    "        self.b = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        self.alpha = np.zeros(n)\n",
    "        self.b = 0\n",
    "        K = polynomial_kernel(X, X, self.degree, self.coef0)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            margin = np.dot((self.alpha * y), K) + self.b\n",
    "            \n",
    "            for i in range(n):\n",
    "                condition = y[i] * margin[i] < 1\n",
    "                \n",
    "                # Subgradient method for the dual\n",
    "                if condition:\n",
    "                    self.alpha[i] += self.lr * (1 - self.C * self.alpha[i])\n",
    "                else:\n",
    "                    self.alpha[i] += self.lr * (-self.C * self.alpha[i])\n",
    "\n",
    "                self.alpha[i] = np.clip(self.alpha[i], 0, self.C)\n",
    "\n",
    "            self.b -= self.lr * np.mean(y * (margin < 1))\n",
    "\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def project(self, X):\n",
    "        K = polynomial_kernel(X, self.X, self.degree, self.coef0)\n",
    "        return np.dot(self.alpha * self.y, K.T) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n",
    "\n",
    "model = PolynomialSVM(C=1.0, lr=0.0001, n_iters=500, degree=3, coef0=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_scores = model.project(X_test)  # For ROC curve\n",
    "\n",
    "tp = np.sum((y_test == 1) & (y_pred == 1))\n",
    "tn = np.sum((y_test == -1) & (y_pred == -1))\n",
    "fp = np.sum((y_test == -1) & (y_pred == 1))\n",
    "fn = np.sum((y_test == 1) & (y_pred == -1))\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "precision = tp / (tp + fp + 1e-6)\n",
    "recall = tp / (tp + fn + 1e-6)\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "\n",
    "print(\"=== Polynomial SVM (From Scratch) – Breast Cancer Classification ===\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n[[TP={tp}, FP={fp}], [FN={fn}, TN={tn}]]\")\n",
    "\n",
    "def compute_roc(y_true, y_score):\n",
    "    # Sort scores and corresponding true labels\n",
    "    indices = np.argsort(y_score)\n",
    "    y_true_sorted = y_true[indices]\n",
    "    y_score_sorted = y_score[indices]\n",
    "    \n",
    "    tpr_list, fpr_list = [0.0], [0.0]\n",
    "    n_pos = np.sum(y_true == 1)\n",
    "    n_neg = np.sum(y_true == -1)\n",
    "    \n",
    "    tp, fp = 0, 0\n",
    "    \n",
    "    # Iterate through sorted scores from high to low\n",
    "    for i in range(len(y_score_sorted) - 1, -1, -1):\n",
    "        if y_true_sorted[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        \n",
    "        tpr_list.append(tp / (n_pos + 1e-6))\n",
    "        fpr_list.append(fp / (n_neg + 1e-6))\n",
    "        \n",
    "    tpr_list.append(1.0)\n",
    "    fpr_list.append(1.0)\n",
    "    return np.array(fpr_list), np.array(tpr_list)\n",
    "\n",
    "# Compute ROC points\n",
    "fpr, tpr = compute_roc(y_test, y_scores)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='blue', label='Polynomial SVM (Scratch)')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random Chance')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve – SVM with Polynomial Kernel\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76da97-95b6-44f7-bac6-74a788024a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
